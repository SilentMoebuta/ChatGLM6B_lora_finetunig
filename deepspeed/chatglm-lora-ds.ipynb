{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9cb77b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:44:27.315543Z",
     "start_time": "2023-04-25T06:44:20.281037Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 06:44:24.661696: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 06:44:24.883031: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-25 06:44:25.681007: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-25 06:44:25.681111: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-25 06:44:25.681122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 这些是deepspeed的设置，单机也要当做多机来设置，要设定master机器，localrank是本机序号，worldsize是总共多少台\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c4a9e",
   "metadata": {},
   "source": [
    "## 读取模型和tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2ed2f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:44:41.153223Z",
     "start_time": "2023-04-25T06:44:27.319082Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f6f7a98b1e41acaf97bcbe3240b196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./chatglm-6b\" \n",
    "# checkpoint = \"THUDM/chatglm-6b\"\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "device = \"cuda\"\n",
    "max_src_length = 512   # 输入最大长度\n",
    "max_dst_length = 512   # 输出最大长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0e372",
   "metadata": {},
   "source": [
    "## 模型转为lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd1f33d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:45:06.879846Z",
     "start_time": "2023-04-25T06:44:41.157166Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py:173: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14680064 || all params: 6187966464 || trainable%: 0.23723567484414862\n"
     ]
    }
   ],
   "source": [
    "def load_lora_config(model):\n",
    "    config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                        inference_mode=False,\n",
    "                        r=32,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.1,\n",
    "                        target_modules=[\"query_key_value\"])\n",
    "    return get_peft_model(model, config)\n",
    "\n",
    "\n",
    "model = load_lora_config(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc381368",
   "metadata": {},
   "source": [
    "## 数据处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa9986d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:45:06.905670Z",
     "start_time": "2023-04-25T06:45:06.887474Z"
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_PATTERN = \"问：{}\"\n",
    "SEP_PATTERN = \"\\n答： \"\n",
    "\n",
    "\n",
    "def create_prompt(question):\n",
    "    return PROMPT_PATTERN.format(question), SEP_PATTERN\n",
    "\n",
    "\n",
    "def create_prompt_ids(tokenizer, question, max_src_length):\n",
    "    prompt, sep = create_prompt(question)\n",
    "    sep_ids = tokenizer.encode(sep, add_special_tokens=True)\n",
    "    sep_len = len(sep_ids)\n",
    "    special_tokens_num = 2\n",
    "    prompt_ids = tokenizer.encode(prompt,\n",
    "                                  max_length=max_src_length -\n",
    "                                  (sep_len - special_tokens_num),\n",
    "                                  truncation=True,\n",
    "                                  add_special_tokens=False)\n",
    "\n",
    "    return prompt_ids + sep_ids\n",
    "\n",
    "\n",
    "def create_inputs_and_labels(tokenizer, question, answer, device):\n",
    "    prompt = create_prompt_ids(tokenizer, question, max_src_length)\n",
    "    completion = tokenizer.encode(answer,\n",
    "                                  max_length=max_dst_length,\n",
    "                                  truncation=True,\n",
    "                                  add_special_tokens=False)\n",
    "\n",
    "    inputs = prompt + completion + [tokenizer.eos_token_id]\n",
    "    labels = [-100] * len(prompt) + completion + [tokenizer.eos_token_id]\n",
    "\n",
    "    inputs = torch.tensor(inputs, dtype=torch.long, device=device)\n",
    "    labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def get_attention_mask(tokenizer, input_ids, device):\n",
    "    seq = input_ids.tolist()\n",
    "    context_len = seq.index(tokenizer.bos_token_id)\n",
    "    seq_len = len(seq)\n",
    "    attention_mask = torch.ones((seq_len, seq_len), device=device)\n",
    "    attention_mask.tril_()\n",
    "    attention_mask[..., :context_len] = 1\n",
    "    attention_mask.unsqueeze_(0)\n",
    "    attention_mask = (attention_mask < 0.5).bool()\n",
    "    return attention_mask\n",
    "\n",
    "\n",
    "def get_position_ids(tokenizer, input_ids, device, position_encoding_2d=True):\n",
    "    seq = input_ids.tolist()\n",
    "    context_len = seq.index(tokenizer.bos_token_id)\n",
    "    seq_len = len(seq)\n",
    "\n",
    "    mask = tokenizer.mask_token_id\n",
    "    gmask = tokenizer.gmask_token_id\n",
    "\n",
    "    mask_token = mask if mask in seq else gmask\n",
    "    use_gmask = False if mask in seq else gmask\n",
    "\n",
    "    mask_position = seq.index(mask_token)\n",
    "\n",
    "    if position_encoding_2d:\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "        if not use_gmask:\n",
    "            position_ids[context_len:] = mask_position\n",
    "        block_position_ids = torch.cat(\n",
    "            (torch.zeros(context_len, dtype=torch.long, device=device),\n",
    "             torch.arange(\n",
    "                 seq_len - context_len, dtype=torch.long, device=device) + 1))\n",
    "        position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n",
    "    else:\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "        if not use_gmask:\n",
    "            position_ids[context_len:] = mask_position\n",
    "\n",
    "    return position_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6dc7f",
   "metadata": {},
   "source": [
    "## 测试用的私有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8512af2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:45:06.913014Z",
     "start_time": "2023-04-25T06:45:06.908348Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data = [\n",
    "#     {\n",
    "#         \"question\": \"为什么 Midjourney 效果远远好于开源的 Stable Diffusion Model?\",\n",
    "#         \"answer\": \"因为题主不会用SD,从门外汉角度得出了错误结论。 Midjourney特点是新手友好,但可控性差、细节优化难。下限高，但上限低。张张都精致唬人，但你想要调节细节时，会发现越调越歪，哪哪都不对劲，抽盲盒一样。\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"核酸检测机构需要什么资质\",\n",
    "#         \"answer\": \"市卫健委将审核申报资料，并结合市区新型冠状病毒核酸检测需求进行综合评估。经评估，具备相应资质和条件的，纳入我市开展新型冠状病毒核酸检测机构名单，并通知进行新型冠状病毒实验活动备案。\"\n",
    "#     },\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49606c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:46:10.937600Z",
     "start_time": "2023-04-25T06:46:06.752827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tsc1.csv')\n",
    "data = []\n",
    "for row in df.values:\n",
    "    if len(row[0]) < 400:\n",
    "        data.append({\"question\": row[0]+\"只回答“是的”或者“不是”。\", \"answer\": \"是的\" if row[1] == 1 else \"不是\"})\n",
    "\n",
    "train_data = data[:20000]\n",
    "eval_data = data[20000:20500]\n",
    "print(len(train_data),len(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646695ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:46:11.561790Z",
     "start_time": "2023-04-25T06:46:11.544750Z"
    }
   },
   "outputs": [],
   "source": [
    "# 包装成dataset\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_data = self.data[index]\n",
    "        tokenizer = self.tokenizer\n",
    "        input_ids, labels = create_inputs_and_labels(\n",
    "            tokenizer,\n",
    "            device=device,\n",
    "            **item_data\n",
    "        )\n",
    "\n",
    "        attention_mask = get_attention_mask(tokenizer, input_ids, device)\n",
    "        position_ids = get_position_ids(tokenizer, input_ids, device)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# 整理函数 在trainer里用的\n",
    "def collate_fn(batch):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    position_ids = []\n",
    "\n",
    "    for obj in batch:\n",
    "        input_ids.append(obj['input_ids'])\n",
    "        labels.append(obj['labels'])\n",
    "        attention_mask.append(obj['attention_mask'])\n",
    "        position_ids.append(obj['position_ids'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_mask),\n",
    "        'labels': torch.stack(labels),\n",
    "        'position_ids': torch.stack(position_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c8190",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739c0478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:46:26.791026Z",
     "start_time": "2023-04-25T06:46:12.771006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-25 06:46:26,660] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    }
   ],
   "source": [
    "# 模型进入gpu\n",
    "model.to(device)\n",
    "\n",
    "# 训练参数设置\n",
    "training_args = TrainingArguments(\"output\",\n",
    "                                  fp16=True,\n",
    "                                  do_train=True,\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  gradient_accumulation_steps=20,\n",
    "                                  learning_rate=1e-4,\n",
    "                                  max_steps=6000,\n",
    "                                  warmup_steps=100,\n",
    "                                  logging_steps=500,\n",
    "                                  do_eval=True,\n",
    "                                  evaluation_strategy='steps',\n",
    "                                  remove_unused_columns=False,\n",
    "                                  seed=114514,\n",
    "                                  data_seed=1919810,\n",
    "                                  group_by_length=False,\n",
    "                                  dataloader_pin_memory=False,\n",
    "                                  save_strategy='steps',\n",
    "                                  save_steps=500,\n",
    "                                  save_total_limit=3,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  deepspeed=\"config/ds_config_zero2.json\")\n",
    "\n",
    "\n",
    "# 设置自己的trainer\n",
    "class ModifiedTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            position_ids=inputs[\"position_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss\n",
    "\n",
    "\n",
    "# 实例化训练数据\n",
    "train_dataset = QADataset(train_data, tokenizer=tokenizer)\n",
    "eval_dataset = QADataset(eval_data, tokenizer=tokenizer)\n",
    "\n",
    "# 实例化trainer\n",
    "trainer = ModifiedTrainer(model=model,\n",
    "                          train_dataset=train_dataset,\n",
    "                          eval_dataset=eval_dataset,\n",
    "                          args=training_args,\n",
    "                          data_collator=collate_fn,\n",
    "                          tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc7024e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:49:06.803242Z",
     "start_time": "2023-04-25T06:46:33.306105Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu116/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 3.4842426776885986 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu116/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load utils op: 0.8580203056335449 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Offload: Total persistent parameters: 1499136 in 226 params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load utils op: 0.001871347427368164 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/6000 00:51 < 85:57:47, 0.02 it/s, Epoch 0.00/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_26888/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4032920361.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_26888/4032920361.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 │   │   </span>inner_training_loop = find_executable_batch_size(                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batc <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 │   │   │   </span>args=args,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 │   │   │   </span>trial=trial,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1929</span> in                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1926 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1927 │   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1928 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1929 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1930 │   │   │   │   </span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1931 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1932 │   │   │   │   │   </span>args.logging_nan_inf_filter                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2699</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2696 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss_mb.reduce_mean().detach().to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.device)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2697 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2698 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss_context_manager():                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2699 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss(model, inputs)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2700 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2701 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.n_gpu &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2702 │   │   │   </span>loss = loss.mean()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># mean() to average on multi-gpu parallel training</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_26888/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2801148213.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">32</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_loss</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_26888/2801148213.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1190</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1187 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1188 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1189 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1190 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/deepspeed/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">nvtx.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">15</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapped_fn</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 │   </span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapped_fn</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 │   │   </span>get_accelerator().range_push(func.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__qualname__</span>)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>15 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>ret_val = func(*args, **kwargs)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 │   │   </span>get_accelerator().range_pop()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ret_val                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">engine.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1695</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1692 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fp16_auto_cast():                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1693 │   │   │   </span>inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._cast_inputs_half(inputs)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1694 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1695 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.module(*inputs, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1696 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1697 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.zero_optimization_partition_weights():                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1698 │   │   │   # Disable automated discovery of external parameters</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1208</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1205 │   │   │   </span>bw_hook = hooks.BackwardHook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, full_backward_hooks)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1206 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span> = bw_hook.setup_input_hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1207 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1208 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1209 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1210 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> hook <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> (*_global_forward_hooks.values(), *<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks.val <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1211 │   │   │   │   </span>hook_result = hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, result)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/peft/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">peft_model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">530</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">527 │   │   </span>**kwargs,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">528 │   </span>):                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">529 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.peft_config, PromptLearningConfig):                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>530 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model(                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">531 │   │   │   │   </span>input_ids=input_ids,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">532 │   │   │   │   </span>attention_mask=attention_mask,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">533 │   │   │   │   </span>inputs_embeds=inputs_embeds,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1208</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1205 │   │   │   </span>bw_hook = hooks.BackwardHook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, full_backward_hooks)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1206 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span> = bw_hook.setup_input_hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1207 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1208 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1209 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1210 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> hook <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> (*_global_forward_hooks.values(), *<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks.val <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1211 │   │   │   │   </span>hook_result = hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, result)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1191</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1188 │   │   </span>use_cache = use_cache <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_cache <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_cache  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1189 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_ <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1190 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1191 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>transformer_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   │   </span>input_ids=input_ids,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   </span>position_ids=position_ids,                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194 │   │   │   </span>attention_mask=attention_mask,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1208</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1205 │   │   │   </span>bw_hook = hooks.BackwardHook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, full_backward_hooks)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1206 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span> = bw_hook.setup_input_hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1207 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1208 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1209 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1210 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> hook <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> (*_global_forward_hooks.values(), *<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks.val <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1211 │   │   │   │   </span>hook_result = hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, result)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">997</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 994 │   │   │   │   │   </span>output_attentions                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 995 │   │   │   │   </span>)                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 996 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 997 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_ret = layer(                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 998 │   │   │   │   │   </span>hidden_states,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 999 │   │   │   │   │   </span>position_ids=position_ids,                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1000 │   │   │   │   │   </span>attention_mask=attention_mask,                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1208</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1205 │   │   │   </span>bw_hook = hooks.BackwardHook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, full_backward_hooks)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1206 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span> = bw_hook.setup_input_hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1207 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1208 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1209 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1210 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> hook <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> (*_global_forward_hooks.values(), *<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks.val <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1211 │   │   │   │   </span>hook_result = hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, result)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">627</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 624 │   │   </span>attention_input = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.input_layernorm(hidden_states)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 625 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 626 │   │   # Self attention.</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 627 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attention_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attention(                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 628 │   │   │   </span>attention_input,                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 629 │   │   │   </span>position_ids,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 630 │   │   │   </span>attention_mask=attention_mask,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1208</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1205 │   │   │   </span>bw_hook = hooks.BackwardHook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, full_backward_hooks)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1206 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span> = bw_hook.setup_input_hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1207 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1208 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1209 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1210 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> hook <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> (*_global_forward_hooks.values(), *<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks.val <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1211 │   │   │   │   </span>hook_result = hook(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, result)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">474</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 471 │   │   │   </span>query_layer, key_layer = apply_rotary_pos_emb_index(query_layer, key_l <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 472 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 473 │   │   # [seq_len, batch, hidden_size]</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 474 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>context_layer, present, attention_probs = attention_fn(                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 475 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 476 │   │   │   </span>query_layer=query_layer,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 477 │   │   │   </span>key_layer=key_layer,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">276</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">attention_fn</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 273 │   # ===================================</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 274 │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 275 │   # [b, np, sq, sk]</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 276 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>output_size = (query_layer.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>), query_layer.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>), query_layer.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>),  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 277 │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 278 │   # [sq, b, np, hn] -&gt; [sq, b * np, hn]</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 279 │   </span>query_layer = query_layer.view(output_size[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>], output_size[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] * output_size[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_26888/\u001b[0m\u001b[1;33m4032920361.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_26888/4032920361.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batc \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1662 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1929\u001b[0m in                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1926 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1927 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1929 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1932 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2699\u001b[0m in \u001b[92mtraining_step\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2696 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m loss_mb.reduce_mean().detach().to(\u001b[96mself\u001b[0m.args.device)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2697 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2698 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.compute_loss_context_manager():                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2699 \u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.compute_loss(model, inputs)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2700 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2701 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.n_gpu > \u001b[94m1\u001b[0m:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2702 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = loss.mean()  \u001b[2m# mean() to average on multi-gpu parallel training\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_26888/\u001b[0m\u001b[1;33m2801148213.py\u001b[0m:\u001b[94m32\u001b[0m in \u001b[92mcompute_loss\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_26888/2801148213.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1190\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1187 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1188 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1189 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1190 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/deepspeed/utils/\u001b[0m\u001b[1;33mnvtx.py\u001b[0m:\u001b[94m15\u001b[0m in \u001b[92mwrapped_fn\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   \u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mwrapped_fn\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   │   \u001b[0mget_accelerator().range_push(func.\u001b[91m__qualname__\u001b[0m)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m15 \u001b[2m│   │   \u001b[0mret_val = func(*args, **kwargs)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   \u001b[0mget_accelerator().range_pop()                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m ret_val                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/\u001b[0m\u001b[1;33mengine.py\u001b[0m:\u001b[94m1695\u001b[0m in \u001b[92mforward\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1692 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.fp16_auto_cast():                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1693 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs = \u001b[96mself\u001b[0m._cast_inputs_half(inputs)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1694 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1695 \u001b[2m│   │   \u001b[0mloss = \u001b[96mself\u001b[0m.module(*inputs, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1696 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1697 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.zero_optimization_partition_weights():                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1698 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Disable automated discovery of external parameters\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1208\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1205 \u001b[0m\u001b[2m│   │   │   \u001b[0mbw_hook = hooks.BackwardHook(\u001b[96mself\u001b[0m, full_backward_hooks)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1206 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m = bw_hook.setup_input_hook(\u001b[96minput\u001b[0m)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1207 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1208 \u001b[2m│   │   \u001b[0mresult = forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1209 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1210 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m (*_global_forward_hooks.values(), *\u001b[96mself\u001b[0m._forward_hooks.val \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1211 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m, result)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/peft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m530\u001b[0m in \u001b[92mforward\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m527 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m528 \u001b[0m\u001b[2m│   \u001b[0m):                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m529 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(\u001b[96mself\u001b[0m.peft_config, PromptLearningConfig):                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m530 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.base_model(                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m531 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids=input_ids,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m532 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattention_mask=attention_mask,                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m533 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs_embeds=inputs_embeds,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1208\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1205 \u001b[0m\u001b[2m│   │   │   \u001b[0mbw_hook = hooks.BackwardHook(\u001b[96mself\u001b[0m, full_backward_hooks)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1206 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m = bw_hook.setup_input_hook(\u001b[96minput\u001b[0m)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1207 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1208 \u001b[2m│   │   \u001b[0mresult = forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1209 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1210 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m (*_global_forward_hooks.values(), *\u001b[96mself\u001b[0m._forward_hooks.val \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1211 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m, result)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m1191\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1188 \u001b[0m\u001b[2m│   │   \u001b[0muse_cache = use_cache \u001b[94mif\u001b[0m use_cache \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_cache  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1189 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_ \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1190 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1191 \u001b[2m│   │   \u001b[0mtransformer_outputs = \u001b[96mself\u001b[0m.transformer(                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=input_ids,                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   \u001b[0mposition_ids=position_ids,                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1194 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1208\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1205 \u001b[0m\u001b[2m│   │   │   \u001b[0mbw_hook = hooks.BackwardHook(\u001b[96mself\u001b[0m, full_backward_hooks)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1206 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m = bw_hook.setup_input_hook(\u001b[96minput\u001b[0m)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1207 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1208 \u001b[2m│   │   \u001b[0mresult = forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1209 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1210 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m (*_global_forward_hooks.values(), *\u001b[96mself\u001b[0m._forward_hooks.val \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1211 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m, result)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m997\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 994 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0moutput_attentions                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 995 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 996 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 997 \u001b[2m│   │   │   │   \u001b[0mlayer_ret = layer(                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 998 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 999 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mposition_ids=position_ids,                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1000 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask=attention_mask,                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1208\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1205 \u001b[0m\u001b[2m│   │   │   \u001b[0mbw_hook = hooks.BackwardHook(\u001b[96mself\u001b[0m, full_backward_hooks)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1206 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m = bw_hook.setup_input_hook(\u001b[96minput\u001b[0m)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1207 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1208 \u001b[2m│   │   \u001b[0mresult = forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1209 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1210 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m (*_global_forward_hooks.values(), *\u001b[96mself\u001b[0m._forward_hooks.val \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1211 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m, result)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m627\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 624 \u001b[0m\u001b[2m│   │   \u001b[0mattention_input = \u001b[96mself\u001b[0m.input_layernorm(hidden_states)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 625 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 626 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Self attention.\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 627 \u001b[2m│   │   \u001b[0mattention_outputs = \u001b[96mself\u001b[0m.attention(                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 628 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_input,                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 629 \u001b[0m\u001b[2m│   │   │   \u001b[0mposition_ids,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 630 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1208\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1205 \u001b[0m\u001b[2m│   │   │   \u001b[0mbw_hook = hooks.BackwardHook(\u001b[96mself\u001b[0m, full_backward_hooks)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1206 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m = bw_hook.setup_input_hook(\u001b[96minput\u001b[0m)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1207 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1208 \u001b[2m│   │   \u001b[0mresult = forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1209 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1210 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m (*_global_forward_hooks.values(), *\u001b[96mself\u001b[0m._forward_hooks.val \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1211 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m, result)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m474\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 471 \u001b[0m\u001b[2m│   │   │   \u001b[0mquery_layer, key_layer = apply_rotary_pos_emb_index(query_layer, key_l \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 472 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 473 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# [seq_len, batch, hidden_size]\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 474 \u001b[2m│   │   \u001b[0mcontext_layer, present, attention_probs = attention_fn(                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 475 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m=\u001b[96mself\u001b[0m,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 476 \u001b[0m\u001b[2m│   │   │   \u001b[0mquery_layer=query_layer,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 477 \u001b[0m\u001b[2m│   │   │   \u001b[0mkey_layer=key_layer,                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m276\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mattention_fn\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 273 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# ===================================\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 274 \u001b[0m\u001b[2m│   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 275 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# [b, np, sq, sk]\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 276 \u001b[2m│   \u001b[0moutput_size = (query_layer.size(\u001b[94m1\u001b[0m), query_layer.size(\u001b[94m2\u001b[0m), query_layer.size(\u001b[94m0\u001b[0m),  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 277 \u001b[0m\u001b[2m│   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 278 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# [sq, b, np, hn] -> [sq, b * np, hn]\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 279 \u001b[0m\u001b[2m│   \u001b[0mquery_layer = query_layer.view(output_size[\u001b[94m2\u001b[0m], output_size[\u001b[94m0\u001b[0m] * output_size[\u001b[94m1\u001b[0m] \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce021ca8",
   "metadata": {},
   "source": [
    "## 保存参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fadbed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:45:11.323406Z",
     "start_time": "2023-04-25T06:45:11.323392Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_tuned_parameters(model, path):\n",
    "    saved_params = {\n",
    "        k: v.to(device)\n",
    "        for k, v in model.named_parameters()\n",
    "        if v.requires_grad\n",
    "    }\n",
    "    torch.save(saved_params, path)\n",
    "\n",
    "model_save_path = 'outputpath'\n",
    "\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "save_tuned_parameters(model, os.path.join(\n",
    "    \"outputpath\", \"chatglm-6b-lora-20230425-tr20000.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c120ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
