{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9cb77b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:44:15.144869Z",
     "start_time": "2023-04-20T11:44:08.208877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 11:44:12.600284: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 11:44:12.774593: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-20 11:44:13.540069: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-20 11:44:13.540178: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-20 11:44:13.540191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c4a9e",
   "metadata": {},
   "source": [
    "## 读取模型和tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2ed2f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:44:30.187615Z",
     "start_time": "2023-04-20T11:44:15.148201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39281b90bd5c4213a8073a712bbb4ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('chatglm-6b'):\n",
    "    checkpoint = \"./chatglm-6b\" # 本地读取模型文件\n",
    "else:\n",
    "    checkpoint = \"THUDM/chatglm-6b\"\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "device = \"cuda\"\n",
    "max_src_length = 256   # 输入最大长度\n",
    "max_dst_length = 256   # 输出最大长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0e372",
   "metadata": {},
   "source": [
    "## 模型转为lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd1f33d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:44:55.419982Z",
     "start_time": "2023-04-20T11:44:30.191925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py:173: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3670016 || all params: 6176956416 || trainable%: 0.05941463324063059\n"
     ]
    }
   ],
   "source": [
    "def load_lora_config(model):\n",
    "    config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                        inference_mode=False,\n",
    "                        r=8,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.1,\n",
    "                        target_modules=[\"query_key_value\"])\n",
    "    return get_peft_model(model, config)\n",
    "\n",
    "\n",
    "model = load_lora_config(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc381368",
   "metadata": {},
   "source": [
    "## 数据处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa9986d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:44:55.438621Z",
     "start_time": "2023-04-20T11:44:55.423516Z"
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_PATTERN = \"问：{}\"\n",
    "SEP_PATTERN = \"\\n答： \"\n",
    "\n",
    "\n",
    "def create_prompt(question):\n",
    "    return PROMPT_PATTERN.format(question), SEP_PATTERN\n",
    "\n",
    "\n",
    "def create_prompt_ids(tokenizer, question, max_src_length):\n",
    "    prompt, sep = create_prompt(question)\n",
    "    sep_ids = tokenizer.encode(sep, add_special_tokens=True)\n",
    "    sep_len = len(sep_ids)\n",
    "    special_tokens_num = 2\n",
    "    prompt_ids = tokenizer.encode(prompt,\n",
    "                                  max_length=max_src_length -\n",
    "                                  (sep_len - special_tokens_num),\n",
    "                                  truncation=True,\n",
    "                                  add_special_tokens=False)\n",
    "\n",
    "    return prompt_ids + sep_ids\n",
    "\n",
    "\n",
    "def create_inputs_and_labels(tokenizer, question, answer, device):\n",
    "    prompt = create_prompt_ids(tokenizer, question, max_src_length)\n",
    "    completion = tokenizer.encode(answer,\n",
    "                                  max_length=max_dst_length,\n",
    "                                  truncation=True,\n",
    "                                  add_special_tokens=False)\n",
    "\n",
    "    inputs = prompt + completion + [tokenizer.eos_token_id]\n",
    "    labels = [-100] * len(prompt) + completion + [tokenizer.eos_token_id]\n",
    "\n",
    "    inputs = torch.tensor(inputs, dtype=torch.long, device=device)\n",
    "    labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def get_attention_mask(tokenizer, input_ids, device):\n",
    "    seq = input_ids.tolist()\n",
    "    context_len = seq.index(tokenizer.bos_token_id)\n",
    "    seq_len = len(seq)\n",
    "    attention_mask = torch.ones((seq_len, seq_len), device=device)\n",
    "    attention_mask.tril_()\n",
    "    attention_mask[..., :context_len] = 1\n",
    "    attention_mask.unsqueeze_(0)\n",
    "    attention_mask = (attention_mask < 0.5).bool()\n",
    "    return attention_mask\n",
    "\n",
    "\n",
    "def get_position_ids(tokenizer, input_ids, device, position_encoding_2d=True):\n",
    "    seq = input_ids.tolist()\n",
    "    context_len = seq.index(tokenizer.bos_token_id)\n",
    "    seq_len = len(seq)\n",
    "\n",
    "    mask = tokenizer.mask_token_id\n",
    "    gmask = tokenizer.gmask_token_id\n",
    "\n",
    "    mask_token = mask if mask in seq else gmask\n",
    "    use_gmask = False if mask in seq else gmask\n",
    "\n",
    "    mask_position = seq.index(mask_token)\n",
    "\n",
    "    if position_encoding_2d:\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "        if not use_gmask:\n",
    "            position_ids[context_len:] = mask_position\n",
    "        block_position_ids = torch.cat(\n",
    "            (torch.zeros(context_len, dtype=torch.long, device=device),\n",
    "             torch.arange(\n",
    "                 seq_len - context_len, dtype=torch.long, device=device) + 1))\n",
    "        position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n",
    "    else:\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "        if not use_gmask:\n",
    "            position_ids[context_len:] = mask_position\n",
    "\n",
    "    return position_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6dc7f",
   "metadata": {},
   "source": [
    "## 测试用的私有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8512af2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:44:55.444497Z",
     "start_time": "2023-04-20T11:44:55.440853Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    {\n",
    "        \"question\": \"为什么 Midjourney 效果远远好于开源的 Stable Diffusion Model?\",\n",
    "        \"answer\": \"因为题主不会用SD,从门外汉角度得出了错误结论。 Midjourney特点是新手友好,但可控性差、细节优化难。下限高，但上限低。张张都精致唬人，但你想要调节细节时，会发现越调越歪，哪哪都不对劲，抽盲盒一样。\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"核酸检测机构需要什么资质\",\n",
    "        \"answer\": \"市卫健委将审核申报资料，并结合市区新型冠状病毒核酸检测需求进行综合评估。经评估，具备相应资质和条件的，纳入我市开展新型冠状病毒核酸检测机构名单，并通知进行新型冠状病毒实验活动备案。\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646695ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:44:55.456123Z",
     "start_time": "2023-04-20T11:44:55.446651Z"
    }
   },
   "outputs": [],
   "source": [
    "# 包装成dataset\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_data = self.data[index]\n",
    "        tokenizer = self.tokenizer\n",
    "        input_ids, labels = create_inputs_and_labels(\n",
    "            tokenizer,\n",
    "            device=device,\n",
    "            **item_data\n",
    "        )\n",
    "\n",
    "        attention_mask = get_attention_mask(tokenizer, input_ids, device)\n",
    "        position_ids = get_position_ids(tokenizer, input_ids, device)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# 整理函数 在trainer里用的\n",
    "def collate_fn(batch):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    position_ids = []\n",
    "\n",
    "    for obj in batch:\n",
    "        input_ids.append(obj['input_ids'])\n",
    "        labels.append(obj['labels'])\n",
    "        attention_mask.append(obj['attention_mask'])\n",
    "        position_ids.append(obj['position_ids'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_mask),\n",
    "        'labels': torch.stack(labels),\n",
    "        'position_ids': torch.stack(position_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c8190",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739c0478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:45:09.800798Z",
     "start_time": "2023-04-20T11:44:55.469076Z"
    }
   },
   "outputs": [],
   "source": [
    "# 模型进入gpu\n",
    "model.to(device)\n",
    "\n",
    "# 训练参数设置\n",
    "training_args = TrainingArguments(\"output\",\n",
    "                                  fp16=True,\n",
    "                                  save_steps=100,\n",
    "                                  save_total_limit=2,\n",
    "                                  gradient_accumulation_steps=1,\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  learning_rate=1e-4,\n",
    "                                  max_steps=300,\n",
    "                                  logging_steps=10,\n",
    "                                  remove_unused_columns=False,\n",
    "                                  seed=114514,\n",
    "                                  data_seed=1919810,\n",
    "                                  group_by_length=False,\n",
    "                                  dataloader_pin_memory=False)\n",
    "\n",
    "# 设置自己的trainer\n",
    "class ModifiedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            position_ids=inputs[\"position_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss\n",
    "\n",
    "# 实例化训练数据\n",
    "train_dataset = QADataset(train_data, tokenizer=tokenizer)\n",
    "\n",
    "# 实例化trainer\n",
    "trainer = ModifiedTrainer(model=model,\n",
    "                          train_dataset=train_dataset,\n",
    "                          args=training_args,\n",
    "                          data_collator=collate_fn,\n",
    "                          tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc7024e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:47:44.636803Z",
     "start_time": "2023-04-20T11:45:09.804105Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:30, Epoch 150/150]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.3944643449022745, metrics={'train_runtime': 151.0658, 'train_samples_per_second': 1.986, 'train_steps_per_second': 1.986, 'total_flos': 868352082739200.0, 'train_loss': 0.3944643449022745, 'epoch': 150.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce021ca8",
   "metadata": {},
   "source": [
    "## 保存参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59fadbed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T11:49:15.651547Z",
     "start_time": "2023-04-20T11:49:15.608269Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_tuned_parameters(model, path):\n",
    "    saved_params = {\n",
    "        k: v.to(device)\n",
    "        for k, v in model.named_parameters()\n",
    "        if v.requires_grad\n",
    "    }\n",
    "    torch.save(saved_params, path)\n",
    "\n",
    "model_save_path = 'outputpath'\n",
    "\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "save_tuned_parameters(model, os.path.join(\n",
    "    \"outputpath\", \"chatglm-6b-lora.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c120ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
